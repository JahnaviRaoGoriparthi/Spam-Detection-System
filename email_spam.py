# -*- coding: utf-8 -*-
"""Email_spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rvTqAPtY2M2LZT7jiqmxm4ZJKrILEF4r
"""

# Mount Google Drive to access files
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
# Set the path to your dataset file
file_path = '/content/drive/MyDrive/Colab Notebooks/NLP/spam.csv'
df = pd.read_csv(file_path,encoding='latin1')
df.head()

df.info()

df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.rename(columns={'v1':'target','v2':'Message'},inplace=True)

df.info()

df.shape

df.isnull().sum()

df.duplicated().sum()

df = df.drop_duplicates()

df.duplicated().sum()

df.shape

df['target'].value_counts()

import matplotlib.pyplot as plt

target_counts = {'ham': 4516, 'spam': 653}

# Create a pie chart
plt.figure(figsize=(8, 6))
plt.pie(target_counts.values(), labels=target_counts.keys(), autopct='%1.1f%%', startangle=140)
plt.title('Target Distribution')
plt.axis('equal')
plt.show()

import nltk
nltk.download('punkt')

df['num_characters'] = df['Message'].apply(len)
df['num_words'] = df['Message'].apply(lambda x:len(nltk.word_tokenize(x)))
df['num_sentences'] = df['Message'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.shape

df.head()

import seaborn as sns

numerical_df = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numerical_df.corr()

sns.set(style='white')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""## **LOWER_CASE  ||   PUNCTUATIONS   ||  STOPWORDS   ||    TOKENIZATION   || STEMMING**"""

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# Download NLTK resources if needed
nltk.download('punkt')
nltk.download('stopwords')

# Initialize the stemmer
ps = PorterStemmer()

# Define the text preprocessing function
def transform_text(text):
    text = text.lower()
    tokens = word_tokenize(text)
    # Remove stopwords, punctuation, and apply stemming
    cleaned_tokens = [ps.stem(token) for token in tokens if token.isalnum() and token not in stopwords.words('english')]
    cleaned_text = " ".join(cleaned_tokens)
    return cleaned_text

df['transformed_text'] = df['Message'].apply(transform_text)
df.head()

from wordcloud import WordCloud

# Separate data for each class
ham_messages = df[df['target'] == 'ham']['Message']
spam_messages = df[df['target'] == 'spam']['Message']

# Function to generate word cloud
def generate_wordcloud(text):
    wordcloud = WordCloud(width=800, height=800, background_color='white').generate(text)
    plt.figure(figsize=(8, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')

# Generate word cloud for 'ham' class
ham_text = ' '.join(ham_messages)
generate_wordcloud(ham_text)
plt.title('Word Cloud for Ham Messages')
plt.show()

# Generate word cloud for 'spam' class
spam_text = ' '.join(spam_messages)
generate_wordcloud(spam_text)
plt.title('\n\n\nWord Cloud for Spam Messages')
plt.show()

from collections import Counter

# Function to identify most repeated tokens in a given class
def most_repeated_tokens(messages, n=10):
    tokens = [word_tokenize(message) for message in messages]
    # Flatten the list of tokens
    flattened_tokens = [token for sublist in tokens for token in sublist]
    token_counts = Counter(flattened_tokens)
    most_common_tokens = token_counts.most_common(n)
    return most_common_tokens

# Identify most repeated tokens in 'ham' class
ham_most_repeated_tokens = most_repeated_tokens(ham_messages)
print("Most repeated tokens in 'ham' class:")
print(ham_most_repeated_tokens)

# Identify most repeated tokens in 'spam' class
spam_most_repeated_tokens = most_repeated_tokens(spam_messages)
print("\nMost repeated tokens in 'spam' class:")
print(spam_most_repeated_tokens)

from collections import Counter

# Separate data for each class
ham_messages = df[df['target'] == 'ham']['transformed_text']
spam_messages = df[df['target'] == 'spam']['transformed_text']

# Function to identify most repeated tokens in a given class
def most_repeated_tokens(messages, n=10):
    tokens = [word_tokenize(message) for message in messages]
    # Flatten the list of tokens
    flattened_tokens = [token for sublist in tokens for token in sublist]
    token_counts = Counter(flattened_tokens)
    most_common_tokens = token_counts.most_common(n)
    return most_common_tokens

# Identify most repeated tokens in 'ham' class
ham_most_repeated_tokens = most_repeated_tokens(ham_messages)
print("Most repeated tokens in 'ham' class:")
print(ham_most_repeated_tokens)

# Identify most repeated tokens in 'spam' class
spam_most_repeated_tokens = most_repeated_tokens(spam_messages)
print("\nMost repeated tokens in 'spam' class:")
print(spam_most_repeated_tokens)

"""# **NORMALIZATION**"""

df['transformed_text']

"""# **spell correction and typo handling**"""

from textblob import TextBlob

# Define a function to perform spell correction and typo handling
def correct_spelling(text):
    blob = TextBlob(text)
    corrected_text = blob.correct()
    return str(corrected_text)

df['corrected_text'] = df['transformed_text'].apply(correct_spelling)

print(df['corrected_text'])

label_map = {'ham': 0, 'spam': 1}
df['binary_target'] = df['target'].map(label_map)

df.to_csv('email_spam111.csv', index=False)



"""# **TF_IDF VECTORIZATION**

term frequency- Inverse document frequency
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

X = df['corrected_text']
y = df['binary_target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Print the shape of the transformed data
print("Shape of X_train_tfidf:", X_train_tfidf.shape)
print("Shape of X_test_tfidf:", X_test_tfidf.shape)

"""

```
# This is formatted as code
```

# ***MACHINE LEARNING MODELS***"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import KFold
import numpy as np

# Initialize classifiers
classifiers = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    "Support Vector Machine": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "k-Nearest Neighbors": KNeighborsClassifier()
}

# Initialize metrics dictionaries
accuracy = {}
precision = {}
recall = {}
roc_auc = {}

# Initialize KFold cross-validator
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Iterate over classifiers
for name, clf in classifiers.items():
    # Initialize lists to store metrics for each fold
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    roc_auc_scores = []

    # Iterate over cross-validation folds
    for train_index, test_index in kf.split(X_train_tfidf):
        X_train_fold, X_test_fold = X_train_tfidf[train_index], X_train_tfidf[test_index]
        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]

        # Train classifier
        clf.fit(X_train_fold, y_train_fold)

        # Predict on test fold
        y_pred_fold = clf.predict(X_test_fold)

        # Calculate metrics for fold
        accuracy_scores.append(accuracy_score(y_test_fold, y_pred_fold))
        precision_scores.append(precision_score(y_test_fold, y_pred_fold))
        recall_scores.append(recall_score(y_test_fold, y_pred_fold))
        roc_auc_scores.append(roc_auc_score(y_test_fold, y_pred_fold))

    # Calculate average metrics across folds
    accuracy[name] = np.mean(accuracy_scores)
    precision[name] = np.mean(precision_scores)
    recall[name] = np.mean(recall_scores)
    roc_auc[name] = np.mean(roc_auc_scores)

# Print metrics for each classifier
for name in classifiers.keys():
    print(f"Classifier: {name}")
    print(f"  Accuracy: {accuracy[name]:.4f}")
    print(f"  Precision: {precision[name]:.4f}")
    print(f"  Recall: {recall[name]:.4f}")
    print(f"  ROC AUC: {roc_auc[name]:.4f}")
    print()

# Initialize classifiers
classifiers = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    "Support Vector Machine": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "k-Nearest Neighbors": KNeighborsClassifier()
}

# Initialize metrics dictionaries
accuracy = {}
precision = {}
recall = {}
roc_auc = {}

kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Iterate over classifiers
for name, clf in classifiers.items():
    # Initialize lists to store metrics for each fold
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    roc_auc_scores = []
    # Iterate over cross-validation folds
    for train_index, test_index in kf.split(X_train_tfidf):
        X_train_fold, X_test_fold = X_train_tfidf[train_index], X_train_tfidf[test_index]
        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]

        # Train classifier
        clf.fit(X_train_fold, y_train_fold)

        # Predict on test fold
        y_pred_fold = clf.predict(X_test_fold)

        # Calculate metrics for fold
        accuracy_scores.append(accuracy_score(y_test_fold, y_pred_fold))
        precision_scores.append(precision_score(y_test_fold, y_pred_fold))
        recall_scores.append(recall_score(y_test_fold, y_pred_fold))
        roc_auc_scores.append(roc_auc_score(y_test_fold, y_pred_fold))

    # Calculate average metrics across folds
    accuracy[name] = np.mean(accuracy_scores)
    precision[name] = np.mean(precision_scores)
    recall[name] = np.mean(recall_scores)
    roc_auc[name] = np.mean(roc_auc_scores)

# Print metrics for each classifier
for name in classifiers.keys():
    print(f"Classifier: {name}")
    print(f"  Accuracy: {accuracy[name]:.4f}")
    print(f"  Precision: {precision[name]:.4f}")
    print(f"  Recall: {recall[name]:.4f}")
    print(f"  ROC AUC: {roc_auc[name]:.4f}")
    print()

"""# **DEEP LEARNING**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from keras.models import Sequential
from keras.layers import Dense, LSTM, GRU, Conv1D, MaxPooling1D, Embedding, Dropout, Flatten
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

# Assuming X_train, y_train are defined

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train_sequences = tokenizer.texts_to_sequences(X_train)

# Define vocabulary size
vocab_size = len(tokenizer.word_index) + 1

# Define embedding dimension
embedding_dim = 100

max_len = 100

# Define neural network models
models = {
    "Feedforward Neural Network": Sequential([
        Dense(64, activation='relu', input_shape=(max_len,)),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ]),
    "Convolutional Neural Network": Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
        Conv1D(128, 5, activation='relu'),
        MaxPooling1D(),
        Flatten(),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ]),
    "Recurrent Neural Network": Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
        LSTM(64),
        Dense(1, activation='sigmoid')
    ]),
    "Long Short-Term Memory Network": Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
        LSTM(64, return_sequences=True),
        LSTM(64),
        Dense(1, activation='sigmoid')
    ]),
    "Gated Recurrent Unit": Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
        GRU(64),
        Dense(1, activation='sigmoid')
    ])
}

# Initialize metrics dictionaries
accuracy = {}
precision = {}
recall = {}
roc_auc = {}

# Initialize KFold cross-validator
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Define a function to train and evaluate a model
def train_and_evaluate_model(model, X_train_fold, y_train_fold, X_test_fold, y_test_fold):
    # Compile model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train model
    model.fit(X_train_fold, y_train_fold, epochs=5, batch_size=32, verbose=0)

    # Predict on test fold
    y_pred_prob_fold = model.predict(X_test_fold)
    y_pred_fold = (y_pred_prob_fold > 0.5).astype(int)

    # Calculate metrics for fold
    accuracy = accuracy_score(y_test_fold, y_pred_fold)
    precision = precision_score(y_test_fold, y_pred_fold)
    recall = recall_score(y_test_fold, y_pred_fold)
    roc_auc = roc_auc_score(y_test_fold, y_pred_fold)

    return accuracy, precision, recall, roc_auc

# Iterate over models
for name, model in models.items():
    print("Training and evaluating model:", name)

    # Initialize lists to store metrics for each fold
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    roc_auc_scores = []

    # Inside the inner loop where you train the model
    for train_index, test_index in kf.split(X_train_sequences):
        X_train_fold = pad_sequences([X_train_sequences[i] for i in train_index], maxlen=max_len)
        X_test_fold = pad_sequences([X_train_sequences[i] for i in test_index], maxlen=max_len)
        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]

        # Train and evaluate the model
        acc, prec, rec, roc = train_and_evaluate_model(model, X_train_fold, y_train_fold, X_test_fold, y_test_fold)

        # Append metrics to lists
        accuracy_scores.append(acc)
        precision_scores.append(prec)
        recall_scores.append(rec)
        roc_auc_scores.append(roc)

    # Calculate average metrics across folds
    accuracy[name] = np.mean(accuracy_scores)
    precision[name] = np.mean(precision_scores)
    recall[name] = np.mean(recall_scores)
    roc_auc[name] = np.mean(roc_auc_scores)

# Print metrics for each model
for name in models.keys():
    print(f"Model: {name}")
    print(f"  Accuracy: {accuracy[name]:.4f}")
    print(f"  Precision: {precision[name]:.4f}")
    print(f"  Recall: {recall[name]:.4f}")
    print(f"  ROC AUC: {roc_auc[name]:.4f}")
    print()

# Save models
for name, model in models.items():
    model.save(f"{name}.h5")

# Save evaluation results to a CSV file
evaluation_results = pd.DataFrame({'Model': list(models.keys()),
                                   'Accuracy': list(accuracy.values()),
                                   'Precision': list(precision.values()),
                                   'Recall': list(recall.values()),
                                   'ROC AUC': list(roc_auc.values())})
evaluation_results.to_csv('evaluation_results.csv', index=False)

